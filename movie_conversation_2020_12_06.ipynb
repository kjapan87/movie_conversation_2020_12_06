{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "movie_conversation_2020-12-06.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kjapan87/movie_conversation_2020_12_06/blob/main/movie_conversation_2020_12_06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIvi2vSq6f8a"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re \n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPxl21ef6m-c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "5d8ec9e9-774e-4d9e-c074-6d369bb02adb"
      },
      "source": [
        "lines = open('movie_lines.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
        "conversations = open('movie_conversations.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-872215fe5092>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'movie_lines.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mconversations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'movie_conversations.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'movie_lines.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBMRsXr96u4F"
      },
      "source": [
        "id2line = {}\n",
        "for line in lines:\n",
        "    _line = line.split(' +++$+++ ')\n",
        "    if len(_line) == 5:\n",
        "        id2line[_line[0]] = _line[4]\n",
        "\n",
        "#in id2line just having line pai\n",
        "conversations_ids = []\n",
        "for conversation in conversations[:-1]:\n",
        "    _conversation = conversation.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n",
        "    conversations_ids.append(_conversation.split(','))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjzpdsan6zkP"
      },
      "source": [
        "questions = []\n",
        "answers = []\n",
        "for conversation in conversations_ids:\n",
        "  for i in range(len(conversation) - 1):\n",
        "    questions.append(id2line[conversation[i]])\n",
        "    answers.append(id2line[conversation[i+1]])  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JNjcLyd649T"
      },
      "source": [
        "#preprocessing function definition\n",
        "def cleanText(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(\"[^a-zA-Z]\",\" \",text)\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"what is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"we'd\", \"we would\", text)    \n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}+=~|.?,]\", \"\", text)\n",
        "    return text\n",
        "    \n",
        "#preprocessing the \"questions\"\n",
        "clean_questions = []\n",
        "for question in questions:\n",
        "    clean_questions.append(cleanText(question))\n",
        "\n",
        "#preprocessing the \"answers\"\n",
        "clean_answers = []\n",
        "for answer in answers:\n",
        "    clean_answers.append(cleanText(answer))    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWKCXDTi69tz"
      },
      "source": [
        "#create a dictionary that map each word to its no. of occurences\n",
        "#we can use many different kind of models , Bag of words, TDIDF, Word2Vec, BERT etc.\n",
        "#however these models are on lexicological level, but we have sequential data, so we created this for loop.\n",
        "word2count = {}\n",
        "for question in clean_questions:\n",
        "    for word in question.split():\n",
        "        if word not in word2count:\n",
        "          word2count[word] = 1\n",
        "        else:\n",
        "          word2count[word] += 1\n",
        "\n",
        "for answer in clean_answers:\n",
        "    for word in answer.split():\n",
        "        if word not in word2count:\n",
        "          word2count[word] = 1\n",
        "        else:\n",
        "          word2count[word] += 1    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33tk6Fhc7KZz"
      },
      "source": [
        "##filtering (aprox.) 5% of extra-words & sorting the occurences to remove repeated entries\n",
        "threshold_ques = 20\n",
        "word_number = 0\n",
        "\n",
        "question2int_dict = {}\n",
        "for word,count in word2count.items():\n",
        "    if count >= threshold_ques:\n",
        "        question2int_dict[word] = word_number\n",
        "        word_number += 1\n",
        "        \n",
        "threshold_ans = 20\n",
        "answers2int_dict = {}\n",
        "for word,count in word2count.items():\n",
        "    if count >= threshold_ans:\n",
        "        answers2int_dict[word] = word_number\n",
        "        word_number += 1    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-jto1Y57TWT"
      },
      "source": [
        "# Adding 4 tags <EOS> (end of sentence) & <SOS> (start of sentence), <padding>, <out> \n",
        "#<OUT> token was created to replace less-frequent word occurence.\n",
        "    \n",
        "tokens = [\"<PAD>\",\"<EOS>\",\"<OUT>\",\"<SOS>\"]\n",
        "\n",
        "for token in tokens:\n",
        "    question2int_dict[token] = len(question2int_dict)+1\n",
        "    \n",
        "for token in tokens:\n",
        "    answers2int_dict[token] = len(answers2int_dict)+1\n",
        "\n",
        "# Inversing a dictionary key:values --> values:key\n",
        "answers2int_wordDict = {word_i:word for word,word_i in answers2int_dict.items()}    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slywDtdk7bx5"
      },
      "source": [
        "\n",
        "#Adding string token to end of every Clean_answers\n",
        "# those 4 tags added above was at word-level and now we again add the tags to sentence-level.\n",
        "#because decoder needs to understand the starting of decoding process (as identifiers)\n",
        "\n",
        "for i in range(len(clean_answers)):\n",
        "    clean_answers[i] += \" <EOS>\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMj7tWyb7iKP"
      },
      "source": [
        "# translating all the questions &answers into integer \n",
        "#using the dictionary that was created and replacing the words to occurence in integers.\n",
        "# & replacing all the words that were filtered out by <OUT> tag.\n",
        "\n",
        "\n",
        "out_questions = []\n",
        "#now the words we removed (5% less frequent word occurrence), how to accomodate these\n",
        "#for these <OUT> words, which were removed, and now again looking \n",
        "#at sentence level, we need the <OUT> tag to replace for those missing part. \n",
        "for question in clean_questions:\n",
        "  integ=[] #using full-sentences & using the created dictionarie\n",
        "  for word in question.split():\n",
        "    if word not in question2int_dict:\n",
        "      integ.append(question2int_dict['<OUT>'])\n",
        "    else:\n",
        "      integ.append(question2int_dict[word])\n",
        "  out_questions.append(integ)\n",
        "\n",
        "out_answers = []\n",
        "\n",
        "for answer in clean_answers:\n",
        "  integ=[] #using full-sentences & using the created dictionaries\n",
        "  for word in answer.split():\n",
        "    if word not in answers2int_dict:\n",
        "      integ.append(answers2int_dict['<OUT>'])\n",
        "    else:\n",
        "      integ.append(answers2int_dict[word])\n",
        "  out_answers.append(integ)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eG43Sx47oOI"
      },
      "source": [
        "#sort all the questions & answers\n",
        "#sort the sentences in question2int which have all the questions as integer\n",
        "#for training puropse, so we sorted the clean out_question as ascending order.\n",
        "# starting from 1 word sentence, to many word sentence.\n",
        "#fetch the \n",
        "\n",
        "sorted_out_question = []\n",
        "sorted_out_answers = []\n",
        "for length in range(1, 10+1):\n",
        "    for i in enumerate(out_questions):\n",
        "        if len(i[1]) == length:\n",
        "            sorted_out_question.append(out_questions[i[0]])\n",
        "            sorted_out_answers.append(out_answers[i[0]])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WY63Nvnc7uKI"
      },
      "source": [
        "#sequences to sequence model\n",
        "\n",
        "##create placeholder (tensors)\n",
        "##input of 4 raw material for building the archtitecture of the model.\n",
        "##input, target, learningrate,keepProb.\n",
        "##in tensorflow all the variables are passed as placeholder(variables in tensorflow)\n",
        "##placeholders are very advanced array.\n",
        "##advanced data-structure\n",
        "\n",
        "def model_input():\n",
        "    inputs = tf.compat.v1.placeholder(tf.int32,[None,None],name = \"input\")\n",
        "    target = tf.compat.v1.placeholder(tf.int32,[None,None],name = \"output\")\n",
        "    learning_rate = tf.compat.v1.placeholder(tf.float32,name = \"learning_rate\")\n",
        "    keep_prob = tf.compat.v1.placeholder(tf.float32,name = \"dropout\")\n",
        "    return inputs,target,learning_rate,keep_prob    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEnSNSaW7u36"
      },
      "source": [
        "#Preprocessing for Decoder as it will predict the answers.\n",
        "##decoder requirement : we require <SOS> in the begining of the sorted_answers\n",
        "##decoder requirement : we require all the sorted_answers in batches\n",
        "\n",
        "def preprocess (target,target2int,batch_size):\n",
        "    left_side = tf.fill([batch_size,1], target2int[\"<SOS>\"], name=None)\n",
        "    ##tf.fill is function is filing an empty array and filling with scalar values\n",
        "    ##tf.fill (left side) will add <SOS>\n",
        "    ##tf.fill (right side) will have all the answers except the last entry (which is <EOS>)\n",
        "    ##Get 2 seperate columns, in the end we join both of these Left & right side together.\n",
        "    right_side = tf.strided_slice(input = target, begin = [0,0], end = [batch_size,-1], strides=[1,1], begin_mask=0, end_mask=0, ellipsis_mask=0,new_axis_mask=0, shrink_axis_mask=0, var=None, name=None)\n",
        "    concatenate_2_side =tf.concat(values = [left_side,right_side], axis = 1)\n",
        "    return concatenate_2_side\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Fncafd377k5"
      },
      "source": [
        "#ENCODER\n",
        "##Encoder won't have training or testing data difference.\n",
        "##will work in a similar way for both training/testing.\n",
        "##batches of questions are going to be fed in the encoder\n",
        "##Encoder\"s basic function is to understand the fed text.\n",
        "\n",
        "def encoder (rnn_input,rnn_size,rnn_layers,keep_prob,sequence_len):\n",
        "    create_LSTM = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "    LSTM_dropout = tf.contrib.rnn.DropoutWrapper(create_LSTM,input_keep_prob = keep_prob)\n",
        "    encoder_cell = tf.contrib.rnn.MultiRNNCell([LSTM_dropout]*rnn_layers)\n",
        "    _,encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_forward = encoder_cell, cell_backwards = encoder_cell,sequence_length = sequence_len,inputs = rnn_input,dtype = tf.float32)\n",
        "    return encoder_state #only encoder state will go to the decoder\n",
        "    ##input_keep_prob is a variable to keep track of keep_prob\n",
        "    ##ecoder is going to be stacked layers\n",
        "    ##bidirectional RNN returns 2 parameters, 1st is encoder_states, 2nd not used\n",
        "    ##\n",
        "    \n",
        "    \n",
        "    #tf.contrib is used to create LSTM cells\n",
        "##rnn_input is the input to the RNN.\n",
        "##rnn_size =  how many number of input tensors of the the encoder.\n",
        "##rnn_layers = no. of layers\n",
        "##keep_prob is to apply dropout regularization, which is used to improve to\n",
        "##control drop-out rate(neurons which we choose to overwrite) so that we can \n",
        "##activate, (usually kept at 20% during training iterations).\n",
        "##sometime dropout rates fluctuates (too high, or too low) so keep_prob will help to control this fluctuation.\n",
        "##sequence_len is the list of length of each question in a batch."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1j2k5Si8HLJ"
      },
      "source": [
        "#DECODER FOR TRAINING DATA\n",
        "def decoder_training_data (encoder_state, decoder_cell, decoder_embedding_input, sequence_len, decoding_scope, output_decoder, keep_prob , batch_size):\n",
        "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size], dtype=tf.dtypes.float32, name=None) #initiallization of weights of attention state\n",
        "    attention_keys, attention_values, attention_score_fn, attention_construct_fn = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"Bahdanau\", num_units = decoder_cell.output_size, reuse=False) #Adding attention features & prepare attention module\n",
        "    ##attention_keys = the keys to be compared to te target state.\n",
        "    ##attention_values = is the value to construct the context vectors. Context is returned by encoder & used by the deceoder.\n",
        "    ##attention_score = is used to compute the similarity between key & target states\n",
        "    ##attention_construct = is used to build the attention states.\n",
        "    ##attention_column is the column of the dataset.\n",
        "    ##decoding_scope will wrap all the variables(contextual_vector, with attention mechanism)\n",
        "    ##Attention are weights, so (not integers but matrix)\n",
        "    ## because there are multiple hidden states, \n",
        "    ##and each hidden layer has a weight attached, so they need to be stored in the form of matrix)\n",
        "    ##attention state, has 3 dimensions, \n",
        "    ##row = batch size, column= 1, elements = output of decoder cell of hidden layer\n",
        "    training_decoder_fn = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0], attention_keys, attention_values, attention_score_fn, attention_construct_fn, name=\"AttentionDecoderTraining\") #pass all features of attention declared in the previous line of code.\n",
        "    decoder_output,_,_ = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell, training_decoder_fn, decoder_embedding_input, sequence_len, decoding_scope, name=\"DecoderOutputforTraining\")\n",
        "    decoder_dropout = tf.nn.dropout(decoder_output, keep_prob, name=\"DecoderDropoutController\")\n",
        "    return decoder_output(decoder_dropout)#helping to updates the weights as it will retrain with feedback mechanism.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sgPNKg_8NZI"
      },
      "source": [
        "#DECODER FOR TESTING DATA\n",
        "\n",
        "##Decoder function of the testing is going to predict the output of the test set.\n",
        "##We will have a Validation set, which will also be given to Training data.\n",
        "##But also Cross-Validation will be done, which is a technique where we keep small part of ou data \n",
        "##to test the predictive power of of model's observation. It is useful to remove overfitting, & \n",
        "##also improves the accuracy on the new observations of the new data.\n",
        "def decoder_testing_data (encoder_state, decoder_cell, decoder_embedding_input, start_of_sequence_id, end_of_sequence_id, maximum_length, num_decoder_symbols, sequence_len, decoding_scope, output_fn, keep_prob , batch_size):\n",
        "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size], dtype=tf.dtypes.float32, name=None) #initiallization of weights of attention state\n",
        "    attention_keys, attention_values, attention_score_fn, attention_construct_fn = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = \"Bahdanau\", num_units = decoder_cell.output_size, reuse=False) #Adding attention features & prepare attention module\n",
        "    testing_decoder_fn = tf.contrib.seq2seq.attention_decoder_fn_inference(output_fn, encoder_state[0], attention_keys, attention_values, attention_score_fn, attention_construct_fn, decoder_embedding_input, start_of_sequence_id, end_of_sequence_id, maximum_length, num_decoder_symbols, dtype=tf.int32, name=None) #maximum_length is the length of longest answer it can find in the batch.#num_decoder_symbol is the total numbers of words in the answers, we need to the answers2int_wordDict.\n",
        "    ##once the chatbot is trained, a logic inside its brain exist, & therefore, able to deduce logically\n",
        "    ##the answers that is being asked. So  it makes its own logic during the training phases\n",
        "    ##and during the inference phase, it will use this logic and use it during testing\n",
        "    test_predictions,_,_ = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell, testing_decoder_fn, decoding_scope, name=\"DecoderOutputforTesting\")\n",
        "    return test_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc4b0iQ98UDB"
      },
      "source": [
        "def decoder (decoder_embedding_input,decoder_embedding_matrix, encoder_state, num_decoder_symbols, rnn_size, rnn_layers, target2int, keep_prob, sequence_len, batch_size):\n",
        "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
        "        create_LSTM = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "        LSTM_dropout = tf.contrib.rnn.DropoutWrapper(create_LSTM,input_keep_prob = keep_prob)\n",
        "        decoder_cell = tf.contrib.rnn.MultiRNNCell([LSTM_dropout]*rnn_layers)##we need to initialize some weights, that will be associated with the neurons of fully-connected layer(LAST LAYER of RNN)\n",
        "        weights = tf.truncated_normal_initializer(mean=0.0, stddev=1.0, seed=None, dtype=tf.dtypes.float32)\n",
        "        bias = tf.zeros_initializer()\n",
        "        output_fully_connected_layer_fn = lambda x: tf.contrib.layers.fully_connected(x,num_decoder_symbols,activation_fn=tf.nn.relu, normalizer_fn=None,normalizer_params=None,weights_initializer= weights,weights_regularizer=None,biases_initializer=bias,biases_regularizer=None,reuse=None,variables_collections=None,outputs_collections=None,trainable=True,scope=decoding_scope)\n",
        "        ##lambda x was used because x can be user defined (text entry in chatbot)\n",
        "        \n",
        "        training_predictions = decoder_training_data (encoder_state, decoder_cell, decoder_embedding_input, sequence_len, decoding_scope, output_decoder, keep_prob , batch_size)\n",
        "        decoding_scope.reuse_variables()\n",
        "        testing_predictions = decoder_testing_data (encoder_state, decoder_cell, decoder_embedding_input, start_of_sequence_id, end_of_sequence_id, maximum_length, num_decoder_symbols, sequence_len-1, decoding_scope, output_fn, keep_prob , batch_size)\n",
        "        \n",
        "        return training_predictions,testing_predictions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqnTD-228YLB"
      },
      "source": [
        "##Combining the Encoder & Decoder (Sequence to Sequence Model)\n",
        "def seq2seq (inputs,target,keep_prob,sequence_len, batch_size, num_words_ques,num_words_anw,encoder_emb_size,decoder_emb_size,rnn_size,rnn_layers,quest_word2int):\n",
        "    ## ASSEMBLAGE ## \n",
        "    encoder_embedded_input =tf.contrib.layers.embed_sequence(inputs,vocab_size=num_words_anw+1,embed_dim=encoder_emb_size,unique=False,initializer=tf.random_uniform_initializer(minval=0, maxval=1, seed=None),regularizer=None,trainable=True,scope=None,reuse=None)#tensorflow will automatically build\n",
        "    encoder_state = encoder(encoder_embedded_input, rnn_size,rnn_layers,keep_prob,sequence_len)\n",
        "    concatenate_2_side = preprocess (target,quest_word2int,batch_size)\n",
        "    decoder_embedding_matrix = tf.Variable(tf.random.uniform([num_words_ques+1,decoder_emb_size], minval=0, maxval=1, dtype=tf.dtypes.float32, seed=None, name=None))\n",
        "    decoder_embedding_input = tf.nn.embedding_lookup(decoder_embedding_matrix, concatenate_2_side, max_norm=None, name=None)\n",
        "    training_predictions,testing_predictions = decoder (decoder_embedding_input,decoder_embedding_matrix, encoder_state, num_words_ques, rnn_size, rnn_layers, quest_word2int, keep_prob, sequence_len, batch_size)\n",
        "    return training_predictions,testing_predictions\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef-O1NHf8ejP"
      },
      "source": [
        "\n",
        "\n",
        "#TRAINING THE SEQ2SEQ MODEL\n",
        "\n",
        "##1) Setting the hyperparameter\n",
        "\n",
        "epoch = 10\n",
        "batch_size = 64\n",
        "rnn_size = 512 ##no. of neurons in 1 RNN cell\n",
        "rnn_layers = 3\n",
        "encoder_emb_size = 512\n",
        "decoder_emb_size = 512\n",
        "learning_rate = 0.01 ##if this learning is to be too high then model is going to learn to fast, which isn't optimum (not too high or too low), the amount by which the weights will be updated aka step-size.(usually 0.0 to 1.0 ) \n",
        "learning_rate_decay = 0.90 ## learning_rate is going to be reduced after iteration of the training, so that the model can be learn deeply the logic.\n",
        "min_learning_rate = 0.0001\n",
        "keep_prob = 0.50 #recommended by geoffery hinton"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Xt1Z6Co8k8x"
      },
      "source": [
        "##2) defining a session, create an object for an interactive session class\n",
        "    ##which is going to open interactive session for interactive flow\n",
        "    ##we also need to reset the tensorflow graph to ensure, that the graph is ready for training \n",
        "    ##graph = in tensorflow a graph is made for all the calling function, and the flow of the calling (return values from 1 block to another)\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "tf.compat.v1.reset_default_graph()\n",
        "session = tf.compat.v1.InteractiveSession()\n",
        "\n",
        "##3) taking model input\n",
        "    \n",
        "inputs,target,learning_rate,keep_prob = model_input()\n",
        "\n",
        "##4) setting the sequence length\n",
        "\n",
        "sequence_len = tf.compat.v1.placeholder_with_default(input = 25 , shape = None, name='sequencelength')\n",
        "\n",
        "##5) Defining the Input Tensors\n",
        "\n",
        "tf.shape(inputs, out_type=tf.dtypes.int32, name=None)\n",
        "\n",
        "\n",
        "##6) Getting the training & test predictions\n",
        "#training_predictions,testing_predictions = seq2seq (decoder_embedding_input,decoder_embedding_matrix, encoder_state, num_words_ques, rnn_size, rnn_layers, quest_word2int, keep_prob, sequence_len, batch_size)\n",
        "\n",
        "#training_predictions,testing_predictions=seq2seq (inputs,target,keep_prob,sequence_len, batch_size, num_words_ques,num_words_anw,encoder_emb_size,decoder_emb_size,rnn_size,rnn_layers,quest_word2int)\n",
        " \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}